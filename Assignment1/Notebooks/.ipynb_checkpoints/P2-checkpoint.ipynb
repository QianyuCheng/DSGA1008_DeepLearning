{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(\\theta)$ be the Objective function the descent algorithm trying to minimize. Define $0 \\leq \\mu \\leq 1$ to be the momentum coefficient, $\\epsilon>0$ be the learning rate and $v_t$ be the velocity vector at $step=t$. \n",
    "\n",
    "Using momentum method, the velocity vector $v_t$ and parameters $\\theta$ are updated by:\n",
    "$$\n",
    "\\begin{split}\n",
    "v_{t+1} &= \\mu v_t-\\epsilon \\triangledown f(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t +v_{t+1}.\n",
    "\\end{split}\n",
    "$$\n",
    "Using Nesterov's Accelerated Gradient(NAG) method, the velocity vector $v_t$ and parameters $\\theta$ are updated by:\n",
    "$$\n",
    "\\begin{split}\n",
    "v_{t+1} &= \\mu v_t-\\epsilon \\triangledown f(\\theta_t+\\mu v_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t +v_{t+1}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Momentum and NAG method differs in how they update velocity vector $v_t$. The NAG method adjust $v_t$ with the gradient of $\\theta +\\mu v_t$ instead of only $\\theta$. In performance, NAG converges more stably than Momentum method since $\\theta +\\mu v_t$ is closer to $\\theta_{t+1}$ than $\\theta_t$ is, $\\triangledown f(\\theta_t+\\mu v_t)$ provides better correction if $\\mu v_t$ is an undesireble step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Reducing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1\n",
    "Dropout method randomly replaces a portion of the hidden layer neurons with zero at each step. The model with partial hidden layers nerons dropped out can be intepreted as a new model with different dimension of hidden layer. Applying dropout at each steps generally ensembles different models at such hidden layer dimension with equal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2\n",
    "Reported in section 7.3 of [2], if the number of hidden units is held constant, the training classifiation error reach flat region(less than 0.1%) when the keeping rate $p$ is around 0.3, the testing classification error reach flat region(around 1%) when $p$ is around 0.5. For $p>0.8$ the test error increases with $p$. Hence the optimal $p$ is between 0.5 and 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would first artificially expands the training data. For image classification task such as MNIST, the small modification of images, although visually not quite observable, provides variations in pixels distributions that is significant to training. I would expand the training data by rotate, translate and skewing the images in original training set. Using these methods, the augmented data would have 100% corrected label. \n",
    "\n",
    "Secondly we can also leverage augmented training data set by propagating the existing labels to unlabeled dataset. There exists assorted propagation methods which we would discussed in the following work."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
